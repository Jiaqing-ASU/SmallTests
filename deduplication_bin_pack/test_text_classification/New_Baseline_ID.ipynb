{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1be7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#input = np.load('detector_output.npy', allow_pickle=True).item()\n",
    "#input = np.load('detector_output_diff_size_unshared_located_random.npy', allow_pickle=True).item()\n",
    "#input = np.load('detector_output_same_size_unshared_located_random.npy', allow_pickle=True).item()\n",
    "#input = np.load('detector_output_same_size_unshared_located_at_last.npy', allow_pickle=True).item()\n",
    "\n",
    "list_of_tensors = list()\n",
    "\n",
    "input_1 = np.load('civil_trainable.npy', allow_pickle=True).item()\n",
    "block_size_1 = input_1.get('block_size')\n",
    "#unique_blocks_1 = len(input_1.get('list_blocks'))\n",
    "tensor_shapes_1 = input_1.get('blocked_tensor_dimension')\n",
    "tensor_mapping_1 = input_1.get('tensor_mapping')\n",
    "num_tensors_1 = len(tensor_shapes_1)\n",
    "\n",
    "for i in range (num_tensors_1):\n",
    "    tensor_shapes_1[i] = input_1.get('blocked_tensor_dimension')[i]\n",
    "for t in range(num_tensors_1):\n",
    "    first, snd = tensor_shapes_1[t]\n",
    "    l = list()\n",
    "    for i in range(first):\n",
    "        for j in range(snd):\n",
    "            l.append(tensor_mapping_1[t].get((i,j)))\n",
    "    list_of_tensors.append(l)\n",
    "\n",
    "input_2 = np.load('imdb_trainable.npy', allow_pickle=True).item()\n",
    "block_size_2 = input_2.get('block_size')\n",
    "#unique_blocks_2 = len(input_2.get('list_blocks'))\n",
    "tensor_shapes_2 = input_2.get('blocked_tensor_dimension')\n",
    "tensor_mapping_2 = input_2.get('tensor_mapping')\n",
    "num_tensors_2 = len(tensor_shapes_2)\n",
    "\n",
    "for i in range (num_tensors_2):\n",
    "    tensor_shapes_2[i] = input_2.get('blocked_tensor_dimension')[i]\n",
    "for t in range(num_tensors_2):\n",
    "    first, snd = tensor_shapes_2[t]\n",
    "    l = list()\n",
    "    for i in range(first):\n",
    "        for j in range(snd):\n",
    "            l.append(tensor_mapping_2[t].get((i,j)))\n",
    "    list_of_tensors.append(l)\n",
    "\n",
    "input_3 = np.load('imdb_nontrainable.npy', allow_pickle=True).item()\n",
    "block_size_3 = input_3.get('block_size')\n",
    "#unique_blocks_3 = len(input_3.get('list_blocks'))\n",
    "tensor_shapes_3 = input_3.get('blocked_tensor_dimension')\n",
    "tensor_mapping_3 = input_3.get('tensor_mapping')\n",
    "num_tensors_3 = len(tensor_shapes_3)\n",
    "\n",
    "for i in range (num_tensors_3):\n",
    "    tensor_shapes_3[i] = input_3.get('blocked_tensor_dimension')[i]\n",
    "for t in range(num_tensors_3):\n",
    "    first, snd = tensor_shapes_3[t]\n",
    "    l = list()\n",
    "    for i in range(first):\n",
    "        for j in range(snd):\n",
    "            l.append(tensor_mapping_3[t].get((i,j)))\n",
    "    list_of_tensors.append(l)\n",
    "\n",
    "input_4 = np.load('yelp_trainable.npy', allow_pickle=True).item()\n",
    "block_size_4 = input_4.get('block_size')\n",
    "#unique_blocks_4 = len(input_4.get('list_blocks'))\n",
    "tensor_shapes_4 = input_4.get('blocked_tensor_dimension')\n",
    "tensor_mapping_4 = input_4.get('tensor_mapping')\n",
    "num_tensors_4 = len(tensor_shapes_4)\n",
    "\n",
    "for i in range (num_tensors_4):\n",
    "    tensor_shapes_4[i] = input_4.get('blocked_tensor_dimension')[i]\n",
    "for t in range(num_tensors_4):\n",
    "    first, snd = tensor_shapes_4[t]\n",
    "    l = list()\n",
    "    for i in range(first):\n",
    "        for j in range(snd):\n",
    "            l.append(tensor_mapping_4[t].get((i,j)))\n",
    "    list_of_tensors.append(l)\n",
    "\n",
    "num_tensors = num_tensors_1 + num_tensors_2 + num_tensors_3 + num_tensors_4\n",
    "\n",
    "#print(list_of_tensors)\n",
    "#print(num_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b257986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "import math\n",
    "from bin_pack import *\n",
    "\n",
    "from numpy.lib.arraysetops import isin\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "import hashlib\n",
    "import numpy as np\n",
    "\n",
    "def bin_pack_base_id(T, l, num_blocks):\n",
    "    I = set()\n",
    "    for t_i in T:\n",
    "        I = I.union(t_i)\n",
    "    I = list(I)\n",
    "    \n",
    "    #items = T[0]\n",
    "\n",
    "    i, j = 0, 0\n",
    "    p_i_j = BinPackingScheme(I, l)\n",
    "    \n",
    "    print(len(I))\n",
    "    # Process at all items in t0\n",
    "    #for i in range(1, len(items) + 1):\n",
    "        # Use 1-index according to logic\n",
    "        #j = I.index(items[i - 1]) + 1\n",
    "        #s = math.ceil(i / l)\n",
    "        #p_i_j.mark(j, s)\n",
    "        \n",
    "    for i in range(1, num_blocks):\n",
    "        j = i + 1\n",
    "        s = math.ceil(i / l) + 1\n",
    "        p_i_j.mark(j, s)\n",
    "    \n",
    "    numBins = math.ceil(num_blocks / l)\n",
    "\n",
    "    # Already added tensor t1\n",
    "    for i in range(2, len(T) + 1):\n",
    "        bin_set, used_bin = p_i_j.findMinBinsMaxCover(T[i - 1],l)\n",
    "        I_delta = set(T[i - 1]) - bin_set\n",
    "        #print(\"I_delta\")\n",
    "        #print(I_delta)\n",
    "        I_delta = list(I_delta)\n",
    "\n",
    "        if not I_delta:\n",
    "            continue\n",
    "        else:\n",
    "            remaining_items = order_tensor_blocks_by_freq(T, I_delta)\n",
    "            #print(remaining_items)\n",
    "            for j in range(1, len(remaining_items) + 1):\n",
    "                # Important to index using I because we built BinPackingScheme using ordering of blocks in I\n",
    "                s = I.index(remaining_items[j - 1]) + 1\n",
    "                u = numBins + math.ceil(j / l)\n",
    "                p_i_j.mark(s, u)\n",
    "\n",
    "            numBins = numBins + math.ceil(len(remaining_items) / l)\n",
    "            #print(numBins)\n",
    "            p_i_j.numBins = numBins\n",
    "\n",
    "    return set([p_i_j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510408b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684\n",
      "Time:  1.7776000579997344\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "blocks_in_page = 5 # page can have 10 blocks\n",
    "P = set()\n",
    "#list_of_tensors = order_tensors_by_small_size(list_of_tensors)\n",
    "start = timeit.default_timer()\n",
    "#P, tensor_page_mapping = bin_pack_greedy(list_of_tensors, blocks_in_page)\n",
    "P = bin_pack_base_id(list_of_tensors, blocks_in_page,684)\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) \n",
    "L = list(P)\n",
    "print(L[0].numBins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7b153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
